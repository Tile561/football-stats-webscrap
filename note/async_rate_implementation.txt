import asyncio
import aiohttp
import random
import time
import pandas as pd
from bs4 import BeautifulSoup, Comment
import warnings

MAX_REQUESTS_PER_MIN = 10
REQUEST_INTERVAL = 60 / MAX_REQUESTS_PER_MIN  # 6 seconds
MAX_CONCURRENT = 3
MAX_RETRIES = 3

semaphore = asyncio.Semaphore(MAX_CONCURRENT)

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 "
    "(KHTML, like Gecko) Version/15.6 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36",
]

def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept-Language": "en-US,en;q=0.9",
        "Accept": "text/html,application/xhtml+xml",
        "Referer": "https://google.com",
    }

async def scrape_player_stats(name, url, table_id, session, retries=MAX_RETRIES):
    warnings.filterwarnings("ignore")
    headers = get_headers()

    async with semaphore:
        # Throttle request rate
        await asyncio.sleep(random.uniform(REQUEST_INTERVAL, REQUEST_INTERVAL + 1))

        try:
            async with session.get(url, headers=headers, timeout=20) as response:
                status = response.status
                print(status, url)

                if status in [403, 429]:
                    if retries > 0:
                        retry_after = int(response.headers.get("Retry-After", 10))
                        print(f"Rate limited ({status}) at {url}, retrying in {retry_after}s")
                        await asyncio.sleep(retry_after + random.uniform(1, 3))
                        return await scrape_player_stats(name, url, table_id, session, retries - 1)
                    else:
                        print(f"Max retries exceeded for {url}")
                        return pd.DataFrame()

                html = await response.text()

        except asyncio.TimeoutError:
            print(f"Timeout for {url}")
            return pd.DataFrame()
        except aiohttp.ClientError as e:
            print(f"Request failed for {url}: {e}")
            return pd.DataFrame()

    # Parse table synchronously (HTML parsing is CPU-bound anyway)
    soup = BeautifulSoup(html, 'html.parser')
    table = soup.find("table", id=table_id) if table_id else soup.find("table", id=None)

    if not table:
        # Check for commented tables
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            if table_id and table_id in comment:
                comment_soup = BeautifulSoup(comment, 'html.parser')
                table = comment_soup.find('table', id=table_id)
                if table:
                    print(f"Found {table_id} in comment for {name}")
                    break

    if not table:
        print(f"Table not found for {name} ({table_id})")
        return pd.DataFrame()

    try:
        df = pd.read_html(str(table), header=1)[0]
        df['League'] = name
        return df
    except Exception as e:
        print(f"Error parsing table for {name}: {e}")
        return pd.DataFrame()

async def main(url_jobs):
    async with aiohttp.ClientSession() as session:
        tasks = [
            scrape_player_stats(job["name"], job["url"], job["table_id"], session)
            for job in url_jobs
        ]
        results = await asyncio.gather(*tasks)
        return results

# Example usage
if __name__ == "__main__":
    jobs = [
        {"name": "Premier League", "url": "https://fbref.com/en/comps/9/2024-2025/possession/2024-2025-Premier-League-Stats", "table_id": "stats_possession"},
        {"name": "La Liga", "url": "https://fbref.com/en/comps/12/2024-2025/possession/2024-2025-La-Liga-Stats", "table_id": "stats_possession"},
        # Add more jobs...
    ]

    results = asyncio.run(main(jobs))
    final_df = pd.concat([df for df in results if not df.empty], ignore_index=True)
    print(final_df.head())
